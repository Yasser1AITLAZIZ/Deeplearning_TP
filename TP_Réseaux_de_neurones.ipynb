{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/Machine-learning/blob/main/TP_R%C3%A9seaux_de_neurones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaC4NDoB40-f"
      },
      "source": [
        "# Entrainement d'un réseau de neurones sur les données MNIST\n",
        "\n",
        "Dans ce TP, nous allons entraîner un réseau de neurones from scratch sur le dataset MNIST, qui contient des images de chiffres manuscrits, pour une tâche de classification.\n",
        "\n",
        "Le réseau de neurones utilisé est de type feedforward (sans récurrence). Il s'agit de l'architecture la plus basique pour les réseaux de neurones.\n",
        "\n",
        "## Architecture du réseau de neurones\n",
        "\n",
        "Le réseau de neurones utilisé est structuré de la manière suivante :   \n",
        "\n",
        "* Couche d'Entrée : La couche d'entrée est composée de 784 neurones. Cette taille correspond au nombre de pixels dans chaque image du dataset MNIST (28x28 pixels). Chaque neurone de cette couche représente l'intensité d'un pixel de l'image en entrée.\n",
        "\n",
        "* Couche Cachée : Le réseau comprend une seule couche cachée. Dans cet exemple, elle est composée de 100 neurones. Ce nombre n'est pas fixe et peut être ajusté en fonction des besoins de complexité du modèle. La couche cachée permet au réseau d'apprendre des représentations plus profondes et des motifs complexes dans les données.\n",
        "\n",
        "* Couche de Sortie : La couche de sortie contient 10 neurones, correspondant aux 10 classes possibles des chiffres (0 à 9) dans le dataset MNIST. Chaque neurone produit une sortie qui représente la probabilité que l'image en entrée appartienne à l'une des 10 classes.\n",
        "\n",
        "## Fonction d'activation\n",
        "\n",
        "La fonction d'activation utilisée dans ce réseau est la fonction sigmoid. Elle est appliquée à chaque neurone de la couche cachée et de la couche de sortie. Cette fonction transforme les valeurs d'entrée en une sortie comprise entre 0 et 1, ce qui est utile pour des problèmes de classification binaire. Dans le cas de MNIST, bien que la classification soit multi-classes, la fonction sigmoid est toujours applicable pour déterminer la probabilité d'appartenance à chaque classe.\n",
        "\n",
        "## Propagation avant\n",
        "\n",
        "Le processus de propagation avant (feedforward) dans un réseau de neurones est une séquence d'opérations linéaires et non-linéaires. Pour notre réseau avec une couche cachée, le processus peut être décrit comme suit :\n",
        "\n",
        "1. **Entrée à la couche cachée** :\n",
        "   - Chaque neurone dans la couche cachée reçoit une combinaison linéaire des entrées :\n",
        "   $h = W^{(1)}x + b^{(1)}$\n",
        "   où $x$ est le vecteur d'entrée (les pixels de l'image), $W^{(1)}$ est la matrice des poids entre la couche d'entrée et la couche cachée, et $b^{(1)}$ est le vecteur de biais de la couche cachée.\n",
        "\n",
        "2. **Activation de la couche cachée** :\n",
        "   - Ensuite, une fonction d'activation non-linéaire est appliquée à chaque élément du vecteur résultant. Dans notre cas, nous utilisons la fonction sigmoid, $\\sigma$ définie par :\n",
        "   $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "   Ainsi, l'activation de la couche cachée est donnée par :\n",
        "   $a^{(1)} = \\sigma(h)$\n",
        "\n",
        "3. **Entrée à la couche de sortie** :\n",
        "   - De manière similaire, les neurones de la couche de sortie reçoivent une combinaison linéaire des activations de la couche cachée :\n",
        "   $o = W^{(2)}a^{(1)} + b^{(2)}$\n",
        "   où $W^{(2)}$ est la matrice des poids entre la couche cachée et la couche de sortie, et $b^{(2)}$ est le vecteur de biais de la couche de sortie.\n",
        "\n",
        "4. **Activation de la couche de sortie** :\n",
        "   - Finalement, la fonction sigmoid est appliquée à la sortie pour obtenir la prédiction finale du réseau :\n",
        "   $y_{\\text{pred}} = \\sigma(o)$\n",
        "\n",
        "Ce processus transforme l'entrée brute (les pixels de l'image) en une prédiction de sortie, qui dans le cas du dataset MNIST, est la probabilité que l'image corresponde à chacun des 10 chiffres (0 à 9).\n",
        "\n",
        "## Rétropropagation\n",
        "\n",
        "Le processus de rétropropagation (backpropagation) dans un réseau de neurones est utilisé pour mettre à jour les poids et biais du réseau en fonction de l'erreur de prédiction. Ce processus peut être décrit mathématiquement comme suit :\n",
        "\n",
        "1. **Calcul de l'erreur de sortie** :\n",
        "   - L'erreur de sortie est la différence entre la sortie prédite du réseau et la sortie réelle (valeur attendue).\n",
        "   $\\delta^{(o)} = y_{\\text{réel}} - y_{\\text{pred}}$\n",
        "   où $y_{\\text{réel}}$ est la sortie réelle et $y_{\\text{pred}}$ est la sortie prédite par le réseau.\n",
        "\n",
        "2. **Gradient de l'erreur par rapport à la sortie** :\n",
        "   - Le gradient de l'erreur par rapport à la sortie est calculé en prenant en compte la dérivée de la fonction d'activation (sigmoid dans notre cas). Ce gradient est utilisé pour mettre à jour les poids de la couche de sortie.\n",
        "   $\\Delta o = \\delta^{(o)} \\cdot \\sigma'(o)$\n",
        "   où $\\sigma'(o)$ est la dérivée de la fonction sigmoid.\n",
        "\n",
        "3. **Erreur de la couche cachée** :\n",
        "   - L'erreur pour chaque neurone de la couche cachée est calculée en propageant l'erreur de la couche de sortie en arrière à travers les poids.\n",
        "   $\\delta^{(h)} = \\Delta o \\cdot W^{(2)T}$\n",
        "   où $W^{(2)T}$ est la transposée de la matrice des poids entre la couche cachée et la couche de sortie.\n",
        "\n",
        "4. **Gradient de l'erreur par eapport à la couche cachée** :\n",
        "   - De même, le gradient pour la couche cachée est calculé en utilisant la dérivée de la fonction d'activation.\n",
        "   $\\Delta h = \\delta^{(h)} \\cdot \\sigma'(h)$\n",
        "\n",
        "5. **Mise à jour des poids et des biais** :\n",
        "   - Les poids et biais sont ensuite mis à jour en fonction des gradients calculés, en utilisant un taux d'apprentissage $\\alpha$.\n",
        "   $W^{(2)} = W^{(2)} + \\alpha \\cdot a^{(1)T} \\cdot \\Delta o$\n",
        "\n",
        "     $W^{(1)} = W^{(1)} + \\alpha \\cdot x^T \\cdot \\Delta h$\n",
        "\n",
        "     $b^{(2)} = b^{(2)} + \\alpha \\cdot \\sum \\Delta o$\n",
        "\n",
        "     $b^{(1)} = b^{(1)} + \\alpha \\cdot \\sum \\Delta h$\n",
        "\n",
        "  où $\\sum$ représente la somme sur tous échantillons.\n",
        "\n",
        "Ce processus de rétropropagation est répété pour chaque exemple dans l'ensemble d'entraînement, permettant ainsi au réseau de neurones d'apprendre et d'ajuster ses paramètres pour minimiser l'erreur de prédiction.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTntwaUSxC5k",
        "outputId": "13863f20-e9a9-4d28-be17-1598a94f6fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 completed\n",
            "Epoch 2/10 completed\n",
            "Epoch 3/10 completed\n",
            "Epoch 4/10 completed\n",
            "Epoch 5/10 completed\n",
            "Epoch 6/10 completed\n",
            "Epoch 7/10 completed\n",
            "Epoch 8/10 completed\n",
            "Epoch 9/10 completed\n",
            "Epoch 10/10 completed\n",
            "Test Loss: 0.0483\n",
            "Test Accuracy: 0.9723\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# Définition des fonctions pour le réseau de neurones\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def forward_propagate(X, weights, bias):\n",
        "    hidden_layer_input = np.dot(X, weights[0]) + bias[0]\n",
        "    hidden_layer_activation = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_activation, weights[1]) + bias[1]\n",
        "    output = sigmoid(output_layer_input)\n",
        "    return hidden_layer_activation, output\n",
        "\n",
        "def backward_propagate(X, Y, hidden_layer_activation, output, weights):\n",
        "    ########### Compléter le code ##############\n",
        "    output_error = Y - output\n",
        "    output_delta = output_error*sigmoid_derivative(output)\n",
        "    hidden_error = np.dot(output_delta , weights[1].T)\n",
        "    hidden_delta = hidden_error*sigmoid_derivative(hidden_layer_activation)\n",
        "    ############################################\n",
        "    return hidden_delta, output_delta\n",
        "\n",
        "def update_weights(X, hidden_layer_activation, hidden_delta, output_delta, weights, bias, learning_rate):\n",
        "    X = X.reshape(1, -1)  # Redimensionner X en une matrice à une ligne\n",
        "\n",
        "    ########### Compléter le code ###############\n",
        "    weights[0] += learning_rate * np.dot(X.T,hidden_delta)\n",
        "    weights[1] += learning_rate * np.dot(hidden_layer_activation.T,output_delta)\n",
        "    bias[0] += hidden_delta.sum()\n",
        "    bias[1] += output_delta.sum()\n",
        "    ############################################\n",
        "    return weights, bias\n",
        "\n",
        "\n",
        "# Chargement et préparation des données MNIST avec PyTorch\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=len(train_dataset))\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
        "\n",
        "train_images, train_labels = next(iter(train_loader))\n",
        "test_images, test_labels = next(iter(test_loader))\n",
        "\n",
        "x_train = train_images.numpy().reshape(-1, 28*28)\n",
        "y_train = np.eye(10)[train_labels.numpy()]\n",
        "x_test = test_images.numpy().reshape(-1, 28*28)\n",
        "y_test = np.eye(10)[test_labels.numpy()]\n",
        "\n",
        "# Paramètres du réseau\n",
        "input_size = 784  # Images de 28x28 pixels\n",
        "hidden_size = 100 # Taille de la couche cachée\n",
        "output_size = 10  # 10 classes pour MNIST (0 à 9)\n",
        "learning_rate = 0.1\n",
        "epochs = 10\n",
        "\n",
        "# Initialisation des poids et des biais\n",
        "weights = [\n",
        "    np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size),\n",
        "    np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size)\n",
        "]\n",
        "bias = [np.zeros((1, hidden_size)), np.zeros((1, output_size))]\n",
        "\n",
        "# Boucle d'entraînement\n",
        "for epoch in range(epochs):\n",
        "    for i in range(len(x_train)):\n",
        "        X = x_train[i]\n",
        "        Y = y_train[i]\n",
        "\n",
        "        hidden_layer_activation, output = forward_propagate(X, weights, bias)\n",
        "        hidden_delta, output_delta = backward_propagate(X, Y, hidden_layer_activation, output, weights)\n",
        "        weights, bias = update_weights(X, hidden_layer_activation, hidden_delta, output_delta, weights, bias, learning_rate)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs} completed')\n",
        "\n",
        "# Test du modèle (évaluation sommaire)\n",
        "loss = 0\n",
        "correct = 0\n",
        "for i in range(len(x_test)):\n",
        "    _, output = forward_propagate(x_test[i], weights, bias)\n",
        "    loss += np.sum((y_test[i] - output) ** 2)\n",
        "    correct += int(np.argmax(output) == np.argmax(y_test[i]))\n",
        "\n",
        "loss /= len(x_test)\n",
        "accuracy = correct / len(x_test)\n",
        "\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMbNfBy/Qz1i/SB00blujd5",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "11690c7cf476975ab356f1450b57592f213281f04c992e64d737ecfd042fb365"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
